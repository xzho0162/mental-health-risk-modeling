{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anxiety Risk Prediction Using Classification Models\n",
    "\n",
    "## Overview\n",
    "This notebook builds multi-class classification models to predict anxiety risk levels from survey-based behavioural indicators.\n",
    "\n",
    "The dataset contains approximately 70 variables capturing emotional state, lifestyle patterns, and socioeconomic background. The goal is to develop a robust classifier and surface the predictors most associated with anxiety risk.\n",
    "\n",
    "## Objective\n",
    "- Train and evaluate multi-class classification models for a 5-class anxiety target\n",
    "- Optimise model performance using **Mean F1-Score** (macro-averaged)\n",
    "- Compare Random Forest, GBM, and Multinomial Logistic Regression\n",
    "\n",
    "## Dataset\n",
    "- **Training set**: 594 samples \u00d7 43 variables (including target)\n",
    "- **Test set**: 95 samples \u00d7 42 variables\n",
    "- **Target**: `alwaysAnxious` \u2014 ordinal scale from \u22122 to +2\n",
    "\n",
    "> **Note:** Dataset files are not included in this repository due to licensing restrictions. Update file paths to your local copy before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approach\n",
    "\n",
    "### Key Finding: Simpler Features \u2192 Better Generalisation\n",
    "\n",
    "| Approach | Features | CV F1 | Test F1 | Outcome |\n",
    "|----------|----------|-------|---------|---------|\n",
    "| Aggressive feature engineering | 25+ | 0.4271 | 0.3621 | Overfitting |\n",
    "| Filtered features (remove NZV) | 41 | 0.3941 | 0.511 | Better |\n",
    "| **All original features** | **42** | **0.3941** | **0.511** | **Best** |\n",
    "\n",
    "### Why This Works\n",
    "- **Class imbalance handling**: 5-fold CV with upsampling for minority classes\n",
    "- **Minimal processing**: Random Forest naturally selects important features; manual filtering may discard useful information\n",
    "- **Model comparison**: Three models trained and compared; GBM achieved the highest CV F1\n",
    "\n",
    "### Final Model\n",
    "- **Algorithm**: GBM (n.trees=400, depth=5, shrinkage=0.05)\n",
    "- **Features**: All 42 original predictors, no filtering\n",
    "- **Validation**: 5-fold stratified CV with upsampling\n",
    "- **CV F1**: 0.4102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All supported packages loaded\n",
      "\n",
      " Data loaded\n",
      "  Train: 594 rows \u00d7 43 cols\n",
      "  Test : 95 rows \u00d7 42 cols\n",
      "\n",
      " Target remapped to valid factor levels\n",
      "\n",
      "class_neg2 class_neg1    class_0 class_pos1 class_pos2 \n",
      "       119         87        182        139         67 \n",
      "\n",
      " Factor columns aligned between train and test\n",
      "\n",
      " NA values filled\n",
      "\n",
      " Using ALL original features (no filtering)\n",
      "  Total predictors: 42 \n",
      "\n",
      "=== Training: Random Forest (mtry: 1-5) ===\n",
      "Fine-tuning around mtry=3...\n",
      "\n",
      " RF training complete\n",
      "  Best mtry: 5 \n",
      "  Best F1: 0.3996  (SD: 0.0517 )\n",
      "\n",
      "  mtry        F1  Accuracy\n",
      "1    1 0.2922939 0.3014967\n",
      "2    2 0.3684368 0.3752568\n",
      "3    3 0.3846963 0.3986386\n",
      "4    4 0.3760472 0.3955873\n",
      "5    5 0.3995718 0.4123539\n",
      "\n",
      "=== Training: GBM ===\n",
      " GBM training complete\n",
      "  Fixed params: n.trees=400, depth=5, shrinkage=0.05\n",
      "  F1: 0.4102  (SD: 0.0083 )\n",
      "\n",
      "=== Training: Multinomial Logistic ===\n",
      " Multinom training complete\n",
      "  Best decay: 0.5 \n",
      "  F1: 0.3644  (SD: 0.0551 )\n",
      "\n",
      "========== MODEL COMPARISON ==========\n",
      "\n",
      "            Model  CV_F1     SD\n",
      "RF             RF 0.3996 0.0517\n",
      "GBM           GBM 0.4102 0.0083\n",
      "Multinom Multinom 0.3644 0.0551\n",
      "\n",
      " Best model: GBM \n",
      "   CV F1: 0.4102 \n",
      "\n",
      "Best parameters: n.trees=400, depth=5, shrinkage=0.05 \n",
      "\n",
      "Prediction label distribution (numeric):\n",
      "pred.label\n",
      "-2 -1  0  1  2 \n",
      " 3 16 30 38  8 \n",
      "\n",
      "Submission file created: ClassificationPredictLabel.csv\n",
      "  Predictions: 95 \n",
      "  Classes: -2, -1, 0, 1, 2 \n",
      "\n",
      "========== FINAL SUMMARY ==========\n",
      "\n",
      "Approach: Ultra-Simple (ALL original features, minimal processing)\n",
      "\n",
      "Data:\n",
      "  Training samples: 594 \n",
      "  Number of predictors: 42 (all original, no filtering)\n",
      "  Feature engineering: NONE\n",
      "\n",
      "Cross-Validation Results (5-fold + upsampling):\n",
      "  Random Forest:     0.3996  (mtry= 5 )\n",
      "  GBM:               0.4102 \n",
      "  Multinomial Log:   0.3644 \n",
      "\n",
      "Selected Model:  GBM \n",
      "Expected Kaggle Score: ~ 0.4002  (conservative estimate)\n",
      "\n",
      " Model training complete\n",
      " Predictions exported to ClassificationPredictLabel.csv\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# part 2-Classification (Mean F1 Kaggle)\n",
    "# ULTRA-SIMPLE VERSION \u2013 ALL ORIGINAL FEATURES\n",
    "# No feature engineering, no feature selection \u2013 just raw data\n",
    "############################################################\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "############################################################\n",
    "# STEP 0: Install / load allowed packages\n",
    "############################################################\n",
    "req_pkgs <- c(\"caret\", \"randomForest\", \"gbm\", \"nnet\")\n",
    "\n",
    "for (p in req_pkgs) {\n",
    "  if (!require(p, character.only = TRUE)) {\n",
    "    install.packages(p, repos = \"http://cran.us.r-project.org\")\n",
    "    library(p, character.only = TRUE)\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\" All supported packages loaded\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 1: Load data\n",
    "############################################################\n",
    "train <- read.csv(\"classification_train.csv\", stringsAsFactors = FALSE)\n",
    "test  <- read.csv(\"classification_test.csv\",  stringsAsFactors = FALSE)\n",
    "\n",
    "cat(\" Data loaded\\n\")\n",
    "cat(\"  Train:\", nrow(train), \"rows \u00d7\", ncol(train), \"cols\\n\")\n",
    "cat(\"  Test :\", nrow(test),  \"rows \u00d7\", ncol(test),  \"cols\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 2: Remap target levels to valid factor names\n",
    "############################################################\n",
    "orig_y <- train$alwaysAnxious\n",
    "\n",
    "label_map_forward <- c(\n",
    "  \"-2\" = \"class_neg2\",\n",
    "  \"-1\" = \"class_neg1\",\n",
    "  \"0\"  = \"class_0\",\n",
    "  \"1\"  = \"class_pos1\",\n",
    "  \"2\"  = \"class_pos2\"\n",
    ")\n",
    "\n",
    "label_map_back <- c(\n",
    "  class_neg2 = -2,\n",
    "  class_neg1 = -1,\n",
    "  class_0    =  0,\n",
    "  class_pos1 =  1,\n",
    "  class_pos2 =  2\n",
    ")\n",
    "\n",
    "train$anx_cat <- factor(\n",
    "  label_map_forward[as.character(orig_y)],\n",
    "  levels = label_map_forward[as.character(sort(unique(orig_y)))]\n",
    ")\n",
    "\n",
    "cat(\" Target remapped to valid factor levels\\n\")\n",
    "print(table(train$anx_cat))\n",
    "cat(\"\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 3: Align character/factor columns\n",
    "############################################################\n",
    "char_cols <- names(train)[sapply(train, is.character)]\n",
    "\n",
    "for (col in char_cols) {\n",
    "  train[[col]] <- factor(train[[col]])\n",
    "}\n",
    "\n",
    "for (col in char_cols) {\n",
    "  if (col %in% names(test)) {\n",
    "    test[[col]] <- factor(test[[col]], levels = levels(train[[col]]))\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\" Factor columns aligned between train and test\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 4: Fill NA values ONLY \u2013 NO OTHER PROCESSING\n",
    "############################################################\n",
    "for (col in names(train)) {\n",
    "  if (is.numeric(train[[col]])) {\n",
    "    train[[col]][is.na(train[[col]])] <- 0\n",
    "  }\n",
    "}\n",
    "for (col in names(test)) {\n",
    "  if (is.numeric(test[[col]])) {\n",
    "    test[[col]][is.na(test[[col]])] <- 0\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\" NA values filled\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 5: USE ALL ORIGINAL FEATURES (NO FILTERING)\n",
    "############################################################\n",
    "x_cols <- setdiff(names(train), c(\"alwaysAnxious\", \"anx_cat\"))\n",
    "\n",
    "train_select <- data.frame(train[, x_cols, drop = FALSE], anx_cat = train$anx_cat)\n",
    "test_select  <- test[, x_cols, drop = FALSE]\n",
    "\n",
    "cat(\" Using ALL original features (no filtering)\\n\")\n",
    "cat(\"  Total predictors:\", length(x_cols), \"\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 6: Custom Macro-F1 (Mean F1-Score)\n",
    "############################################################\n",
    "macro_f1 <- function(truth, pred) {\n",
    "  truth <- factor(truth)\n",
    "  pred  <- factor(pred, levels = levels(truth))\n",
    "  cm <- table(truth, pred)\n",
    "  \n",
    "  f1s <- c()\n",
    "  for (k in rownames(cm)) {\n",
    "    tp <- cm[k, k]\n",
    "    fp <- sum(cm[, k]) - tp\n",
    "    fn <- sum(cm[k, ]) - tp\n",
    "    prec <- ifelse(tp + fp == 0, 0, tp/(tp+fp))\n",
    "    rec  <- ifelse(tp + fn == 0, 0, tp/(tp+fn))\n",
    "    f1   <- ifelse(prec + rec == 0, 0, 2*prec*rec/(prec+rec))\n",
    "    f1s <- c(f1s, f1)\n",
    "  }\n",
    "  mean(f1s)\n",
    "}\n",
    "\n",
    "f1_summary <- function(data, lev = NULL, model = NULL) {\n",
    "  f1 <- macro_f1(data$obs, data$pred)\n",
    "  acc <- mean(data$obs == data$pred)\n",
    "  c(F1 = f1, Accuracy = acc)\n",
    "}\n",
    "\n",
    "############################################################\n",
    "# STEP 7: Cross-validation setup\n",
    "############################################################\n",
    "ctrl <- trainControl(\n",
    "  method = \"cv\",\n",
    "  number = 5,\n",
    "  summaryFunction = f1_summary,\n",
    "  classProbs = TRUE,\n",
    "  savePredictions = \"final\",\n",
    "  sampling = \"up\",\n",
    "  verboseIter = FALSE\n",
    ")\n",
    "\n",
    "############################################################\n",
    "# STEP 8: Train models with optimized parameters\n",
    "# Based on previous findings:\n",
    "# - RF: The best mtry value is 3\n",
    "# - GBM: The optimal parameters are n.trees = 400, depth = 5, shrinkage = 0.05\n",
    "# - Fine-tune with a smaller mtry range (1-5)\n",
    "############################################################\n",
    "\n",
    "## 8.1 Random Forest \u2013 Fine-tune mtry around optimal value\n",
    "cat(\"=== Training: Random Forest (mtry: 1-5) ===\\n\")\n",
    "cat(\"Fine-tuning around mtry=3...\\n\\n\")\n",
    "\n",
    "m_rf <- train(\n",
    "  anx_cat ~ .,\n",
    "  data = train_select,\n",
    "  method = \"rf\",\n",
    "  trControl = ctrl,\n",
    "  tuneGrid = expand.grid(mtry = c(1, 2, 3, 4, 5)),\n",
    "  ntree = 300,\n",
    "  metric = \"F1\"\n",
    ")\n",
    "\n",
    "rf_best_mtry <- m_rf$bestTune$mtry\n",
    "rf_f1 <- max(m_rf$results$F1)\n",
    "rf_sd <- sd(m_rf$resample$F1)\n",
    "\n",
    "cat(\" RF training complete\\n\")\n",
    "cat(\"  Best mtry:\", rf_best_mtry, \"\\n\")\n",
    "cat(\"  Best F1:\", round(rf_f1, 4), \" (SD:\", round(rf_sd, 4), \")\\n\\n\")\n",
    "\n",
    "print(m_rf$results[, c(\"mtry\", \"F1\", \"Accuracy\")])\n",
    "cat(\"\\n\")\n",
    "\n",
    "## 8.2 GBM \u2013 Use optimized parameters from previous run\n",
    "cat(\"=== Training: GBM ===\\n\")\n",
    "m_gbm <- train(\n",
    "  anx_cat ~ .,\n",
    "  data = train_select,\n",
    "  method = \"gbm\",\n",
    "  trControl = ctrl,\n",
    "  verbose = FALSE,\n",
    "  tuneGrid = expand.grid(\n",
    "    n.trees = 400,\n",
    "    interaction.depth = 5,\n",
    "    shrinkage = 0.05,\n",
    "    n.minobsinnode = 10\n",
    "  ),\n",
    "  metric = \"F1\"\n",
    ")\n",
    "\n",
    "gbm_f1 <- max(m_gbm$results$F1)\n",
    "gbm_sd <- sd(m_gbm$resample$F1)\n",
    "\n",
    "cat(\" GBM training complete\\n\")\n",
    "cat(\"  Fixed params: n.trees=400, depth=5, shrinkage=0.05\\n\")\n",
    "cat(\"  F1:\", round(gbm_f1, 4), \" (SD:\", round(gbm_sd, 4), \")\\n\\n\")\n",
    "\n",
    "## 8.3 Multinomial Logistic\n",
    "cat(\"=== Training: Multinomial Logistic ===\\n\")\n",
    "m_mult <- train(\n",
    "  anx_cat ~ .,\n",
    "  data = train_select,\n",
    "  method = \"multinom\",\n",
    "  trControl = ctrl,\n",
    "  trace = FALSE,\n",
    "  tuneGrid = data.frame(decay = c(0, 0.1, 0.5)),\n",
    "  metric = \"F1\"\n",
    ")\n",
    "\n",
    "mult_best_decay <- m_mult$bestTune$decay\n",
    "mult_f1 <- max(m_mult$results$F1)\n",
    "mult_sd <- sd(m_mult$resample$F1)\n",
    "\n",
    "cat(\" Multinom training complete\\n\")\n",
    "cat(\"  Best decay:\", mult_best_decay, \"\\n\")\n",
    "cat(\"  F1:\", round(mult_f1, 4), \" (SD:\", round(mult_sd, 4), \")\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 9: Model comparison\n",
    "############################################################\n",
    "cat(\"========== MODEL COMPARISON ==========\\n\\n\")\n",
    "\n",
    "f1_scores <- c(RF = rf_f1, GBM = gbm_f1, Multinom = mult_f1)\n",
    "best_idx <- which.max(f1_scores)\n",
    "best_name <- names(f1_scores)[best_idx]\n",
    "\n",
    "comparison_df <- data.frame(\n",
    "  Model = names(f1_scores),\n",
    "  CV_F1 = round(f1_scores, 4),\n",
    "  SD = c(round(rf_sd, 4), round(gbm_sd, 4), round(mult_sd, 4))\n",
    ")\n",
    "\n",
    "print(comparison_df)\n",
    "cat(\"\\n\")\n",
    "\n",
    "cat(\" Best model:\", best_name, \"\\n\")\n",
    "cat(\"   CV F1:\", round(max(f1_scores), 4), \"\\n\\n\")\n",
    "\n",
    "if (best_name == \"RF\") {\n",
    "  fin.mod <- m_rf\n",
    "  best_params <- paste(\"mtry=\", rf_best_mtry, \", ntree=300\", sep=\"\")\n",
    "} else if (best_name == \"GBM\") {\n",
    "  fin.mod <- m_gbm\n",
    "  best_params <- \"n.trees=400, depth=5, shrinkage=0.05\"\n",
    "} else {\n",
    "  fin.mod <- m_mult\n",
    "  best_params <- paste(\"decay=\", mult_best_decay, sep=\"\")\n",
    "}\n",
    "\n",
    "cat(\"Best parameters:\", best_params, \"\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 10: Predict on test\n",
    "############################################################\n",
    "test_pred_fac <- predict(fin.mod, newdata = test_select, type = \"raw\")\n",
    "pred.label <- as.numeric(label_map_back[as.character(test_pred_fac)])\n",
    "\n",
    "cat(\"Prediction label distribution (numeric):\\n\")\n",
    "print(table(pred.label))\n",
    "cat(\"\\n\")\n",
    "\n",
    "############################################################\n",
    "# STEP 11: Write submission file\n",
    "############################################################\n",
    "submission <- data.frame(\n",
    "  RowIndex   = 1:length(pred.label),\n",
    "  Prediction = pred.label\n",
    ")\n",
    "\n",
    "write.csv(submission, \"ClassificationPredictLabel.csv\", row.names = FALSE)\n",
    "\n",
    "cat(\"Submission file created: ClassificationPredictLabel.csv\\n\")\n",
    "cat(\"  Predictions:\", length(pred.label), \"\\n\")\n",
    "cat(\"  Classes:\", paste(sort(unique(pred.label)), collapse = \", \"), \"\\n\\n\")\n",
    "\n",
    "############################################################\n",
    "# FINAL SUMMARY\n",
    "############################################################\n",
    "cat(\"========== FINAL SUMMARY ==========\\n\\n\")\n",
    "\n",
    "cat(\"Approach: Ultra-Simple (ALL original features, minimal processing)\\n\\n\")\n",
    "\n",
    "cat(\"Data:\\n\")\n",
    "cat(\"  Training samples:\", nrow(train_select), \"\\n\")\n",
    "cat(\"  Number of predictors:\", length(x_cols), \"(all original, no filtering)\\n\")\n",
    "cat(\"  Feature engineering: NONE\\n\\n\")\n",
    "\n",
    "cat(\"Cross-Validation Results (5-fold + upsampling):\\n\")\n",
    "cat(\"  Random Forest:    \", round(rf_f1, 4), \" (mtry=\", rf_best_mtry, \")\\n\")\n",
    "cat(\"  GBM:              \", round(gbm_f1, 4), \"\\n\")\n",
    "cat(\"  Multinomial Log:  \", round(mult_f1, 4), \"\\n\\n\")\n",
    "\n",
    "cat(\"Selected Model: \", best_name, \"\\n\")\n",
    "cat(\"Expected out-of-sample F1: ~\", round(max(f1_scores) - 0.01, 4), \n",
    "    \" (conservative estimate)\\n\\n\")\n",
    "\n",
    "cat(\" Final model stored in fin.mod\\n\")\n",
    "cat(\" Predictions exported to ClassificationPredictLabel.csv\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}