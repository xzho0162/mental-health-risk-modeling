{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happiness Prediction Using Regression Models\n",
    "\n",
    "## Overview\n",
    "This notebook develops regression models to predict individual happiness levels using large-scale behavioural and demographic survey data.\n",
    "\n",
    "Approximately 70 predictor variables are considered to quantify which factors are most strongly associated with wellbeing and to build a model suitable for generating out-of-sample predictions.\n",
    "\n",
    "## Objective\n",
    "- Build and evaluate regression models for predicting **happiness**\n",
    "- Identify statistically significant predictors (\u03b1 = 0.01) and report the **Top 5** strongest predictors\n",
    "- Compare full model, stepwise BIC, and lightweight 2-predictor model\n",
    "- Produce a reproducible prediction pipeline (data prep \u2192 modelling \u2192 evaluation \u2192 submission)\n",
    "\n",
    "## Dataset\n",
    "- **Training set**: 500 samples \u00d7 43 variables (including target)\n",
    "- **Test set**: 90 samples \u00d7 42 variables\n",
    "- **Target**: `happiness` \u2014 continuous numeric score\n",
    "\n",
    "> **Note:** Dataset files are not included in this repository due to licensing restrictions. Update file paths to your local copy before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Linear Regression \u2014 Significant Predictors\n",
    "\n",
    "Fit a full multiple linear regression model and identify predictors significant at \u03b1 = 0.01, ranked by |t-statistic|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Significant Predictors (p < 0.01) \n",
      "                                                      Estimate Std.Error\n",
      "income80k - 120k                                     37.315988 1.6548496\n",
      "income50k - 80k                                      28.546980 1.7382901\n",
      "income200k above                                     20.109900 1.3788320\n",
      "income20k - 50k                                      20.839607 1.5825100\n",
      "income15k - 20k                                      15.447908 1.2809264\n",
      "income150k - 200k                                    11.718396 1.2434310\n",
      "income120k - 150k                                    11.373025 1.2340393\n",
      "income10k - 15k                                       6.268117 1.2386952\n",
      "whatIsYourHeightExpressItAsANumberInMetresM180 - 185  7.286107 2.3791630\n",
      "alwaysStressed                                       -1.064389 0.3726381\n",
      "whatIsYourHeightExpressItAsANumberInMetresM175 - 180  6.104323 2.2061513\n",
      "                                                       t.value      p.value\n",
      "income80k - 120k                                     22.549474 6.160552e-75\n",
      "income50k - 80k                                      16.422449 2.033722e-47\n",
      "income200k above                                     14.584735 1.902543e-39\n",
      "income20k - 50k                                      13.168704 1.604206e-33\n",
      "income15k - 20k                                      12.059949 4.650658e-29\n",
      "income150k - 200k                                     9.424243 2.676570e-19\n",
      "income120k - 150k                                     9.216096 1.369692e-18\n",
      "income10k - 15k                                       5.060258 6.210721e-07\n",
      "whatIsYourHeightExpressItAsANumberInMetresM180 - 185  3.062466 2.332715e-03\n",
      "alwaysStressed                                       -2.856361 4.492518e-03\n",
      "whatIsYourHeightExpressItAsANumberInMetresM175 - 180  2.766956 5.901767e-03\n",
      "\n",
      " Top 5 Strongest Predictors (by |t|) \n",
      "                 Estimate  t.value      p.value\n",
      "income80k - 120k 37.31599 22.54947 6.160552e-75\n",
      "income50k - 80k  28.54698 16.42245 2.033722e-47\n",
      "income200k above 20.10990 14.58474 1.902543e-39\n",
      "income20k - 50k  20.83961 13.16870 1.604206e-33\n",
      "income15k - 20k  15.44791 12.05995 4.650658e-29\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data\n",
    "v <- read.csv(\"regression_train.csv\", stringsAsFactors = TRUE)\n",
    "\n",
    "# 2) Fit full model\n",
    "model <- lm(happiness ~ ., data = v)\n",
    "\n",
    "# 3) Get summary and coefficient table\n",
    "s <- summary(model)\n",
    "coef_tab <- as.data.frame(s$coefficients)\n",
    "\n",
    "coef_tab <- coef_tab[rownames(coef_tab) != \"(Intercept)\", ]\n",
    "colnames(coef_tab) <- c(\"Estimate\", \"Std.Error\", \"t.value\", \"p.value\")\n",
    "\n",
    "cat(\"\\n Significant Predictors (p < 0.01) \\n\")\n",
    "sig_pred <- coef_tab[coef_tab$p.value < 0.01, ]\n",
    "\n",
    "if (nrow(sig_pred) == 0) {\n",
    "  cat(\"No predictors are significant at p < 0.01\\n\")\n",
    "} else {\n",
    "  # order by p-value ascending\n",
    "  sig_pred <- sig_pred[order(sig_pred$p.value), ]\n",
    "  print(sig_pred)\n",
    "}\n",
    "\n",
    "# 4) Top 5 strongest predictors \n",
    "coef_tab$abs_t <- abs(coef_tab$t.value)\n",
    "# how many predictors do we actually have?\n",
    "k <- min(5, nrow(coef_tab))\n",
    "top_5 <- coef_tab[order(-coef_tab$abs_t), ][1:k, c(\"Estimate\", \"t.value\", \"p.value\")]\n",
    "\n",
    "cat(\"\\n Top\", k, \"Strongest Predictors (by |t|) \\n\")\n",
    "print(top_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RMSE Evaluation Function\n",
    "\n",
    "Implement a reusable function to compute RMSE between predictions and ground truth, and evaluate the full model on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model Performance on Training Data \n",
      "RMSE: 6.672557 \n",
      "R-squared: 0.7685182 \n"
     ]
    }
   ],
   "source": [
    "# 1) Load data and fit model\n",
    "v <- read.csv(\"regression_train.csv\", stringsAsFactors = TRUE)\n",
    "model <- lm(happiness ~ ., data = v)\n",
    "\n",
    "# 2) Function to predict and compute RMSE (only if 'happiness' exists)\n",
    "predict_and_rmse <- function(model, data) {\n",
    "  preds <- predict(model, newdata = data)\n",
    "  \n",
    "  if (!(\"happiness\" %in% names(data))) {\n",
    "    # no actual target, just return predictions\n",
    "    return(list(predictions = preds, rmse = NA))\n",
    "  }\n",
    "  \n",
    "  actual <- data$happiness\n",
    "  rmse <- sqrt(mean((actual - preds)^2))\n",
    "  return(list(predictions = preds, rmse = rmse))\n",
    "}\n",
    "\n",
    "# 3) Evaluate on training set\n",
    "res <- predict_and_rmse(model, v)\n",
    "\n",
    "cat(\" Model Performance on Training Data \\n\")\n",
    "cat(\"RMSE:\", res$rmse, \"\\n\")\n",
    "cat(\"R-squared:\", summary(model)$r.squared, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stepwise Regression with BIC\n",
    "\n",
    "Perform bidirectional stepwise regression using BIC to reduce model complexity, and compare with the full model.\n",
    "\n",
    "**Note:** R\u00b2 on the training set is not always the best metric \u2014 BIC penalises complexity to favour models that generalise better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Comparison of Models \n",
      "Full Model:\n",
      "  RMSE       : 6.672557 \n",
      "  R-squared  : 0.7685182 \n",
      "  #Predictors: 68 \n",
      "\n",
      "Stepwise Model (BIC):\n",
      "  RMSE       : 7.330305 \n",
      "  R-squared  : 0.7206321 \n",
      "  #Predictors: 14 \n",
      "\n",
      "Difference in RMSE (step - full): 0.6577483 \n",
      "Model complexity reduced by: 54 predictors\n",
      "\n",
      "Selected model formula:\n",
      "happiness ~ income + alwaysStressed + alwaysHaveFun + alwaysSerious + \n",
      "    alwaysDepressed + iFindMostThingsAmusing + iUsuallyHaveAGoodInfluenceOnEvents\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data and fit full model\n",
    "v <- read.csv(\"regression_train.csv\", stringsAsFactors = TRUE)\n",
    "model_full <- lm(happiness ~ ., data = v)\n",
    "\n",
    "# 2) Bidirectional stepwise regression with BIC (quiet)\n",
    "model_step <- step(\n",
    "  model_full,\n",
    "  direction = \"both\",\n",
    "  k = log(nrow(v)),\n",
    "  trace = 0\n",
    ")\n",
    "\n",
    "# 3) Reuse function from Q2 to calculate RMSE\n",
    "predict_and_rmse <- function(model, data) {\n",
    "  preds <- predict(model, newdata = data)\n",
    "  if (!(\"happiness\" %in% names(data))) {\n",
    "    return(list(predictions = preds, rmse = NA))\n",
    "  }\n",
    "  actual <- data$happiness\n",
    "  # make sure lengths match\n",
    "  rmse <- sqrt(mean((actual - preds)^2))\n",
    "  return(list(predictions = preds, rmse = rmse))\n",
    "}\n",
    "\n",
    "# 4) Calculate RMSE for both models on the SAME training data\n",
    "res_full <- predict_and_rmse(model_full, v)\n",
    "res_step <- predict_and_rmse(model_step, v)\n",
    "\n",
    "cat(\" Comparison of Models \\n\")\n",
    "\n",
    "cat(\"Full Model:\\n\")\n",
    "cat(\"  RMSE       :\", res_full$rmse, \"\\n\")\n",
    "cat(\"  R-squared  :\", summary(model_full)$r.squared, \"\\n\")\n",
    "cat(\"  #Predictors:\", length(coef(model_full)) - 1, \"\\n\\n\")\n",
    "\n",
    "cat(\"Stepwise Model (BIC):\\n\")\n",
    "cat(\"  RMSE       :\", res_step$rmse, \"\\n\")\n",
    "cat(\"  R-squared  :\", summary(model_step)$r.squared, \"\\n\")\n",
    "cat(\"  #Predictors:\", length(coef(model_step)) - 1, \"\\n\\n\")\n",
    "\n",
    "cat(\"Difference in RMSE (step - full):\", res_step$rmse - res_full$rmse, \"\\n\")\n",
    "cat(\"Model complexity reduced by:\",\n",
    "    (length(coef(model_full)) - length(coef(model_step))), \"predictors\\n\\n\")\n",
    "\n",
    "cat(\"Selected model formula:\\n\")\n",
    "print(formula(model_step))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The stepwise model using BIC reduced from 68 to 14 predictors, but training RMSE increased from 6.67 to 7.33 and R\u00b2 dropped from 0.769 to 0.721. This is expected: BIC explicitly penalises model complexity, prioritising simplicity and generalisability over in-sample fit. The model trades minimal predictive accuracy for interpretability and stability \u2014 retaining only key factors like income, stress, and emotional traits. Simpler models are less prone to overfitting and more robust on unseen data, so the higher training RMSE reflects BIC's fundamental purpose of balancing complexity and predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Best Lightweight Model (2 Predictors)\n",
    "\n",
    "Exhaustive search over all 2-predictor combinations to find the model with the lowest training RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best 2-Predictor Model (exhaustive search) \n",
      "Predictors: income and alwaysStressed \n",
      "RMSE: 7.885411 \n",
      "R-squared: 0.6767183 \n",
      "\n",
      " Stepwise (BIC) Model \n",
      "RMSE: 7.330305 \n",
      "R-squared: 0.7206321 \n",
      "Predictors: 14 \n",
      "\n",
      " Full Model \n",
      "RMSE: 6.672557 \n",
      "R-squared: 0.7685182 \n",
      "Predictors: 68 \n",
      "\n",
      "Difference in RMSE (2-pred - step): 0.5551064 \n",
      "Difference in RMSE (2-pred - full): 1.212855 \n",
      "\n",
      "2-predictor formula:\n",
      "happiness ~ income + alwaysStressed\n"
     ]
    }
   ],
   "source": [
    "# 1) Load data\n",
    "v <- read.csv(\"regression_train.csv\", stringsAsFactors = TRUE)\n",
    "\n",
    "# 2) Full model (for comparison)\n",
    "model_full <- lm(happiness ~ ., data = v)\n",
    "\n",
    "# 3) Stepwise model with BIC (also for comparison, from Q3 idea)\n",
    "model_step <- step(\n",
    "  model_full,\n",
    "  direction = \"both\",\n",
    "  k = log(nrow(v)),\n",
    "  trace = 0\n",
    ")\n",
    "\n",
    "# 4) RMSE function (reuse from Q2)\n",
    "predict_and_rmse <- function(model, data) {\n",
    "  preds <- predict(model, newdata = data)\n",
    "  if (!(\"happiness\" %in% names(data))) {\n",
    "    return(list(predictions = preds, rmse = NA))\n",
    "  }\n",
    "  actual <- data$happiness\n",
    "  rmse <- sqrt(mean((actual - preds)^2))\n",
    "  return(list(predictions = preds, rmse = rmse))\n",
    "}\n",
    "\n",
    "# 5) Exhaustive search for the best 2-predictor model\n",
    "predictor_names <- setdiff(names(v), \"happiness\")\n",
    "n_pred <- length(predictor_names)\n",
    "\n",
    "best_rmse <- Inf\n",
    "best_preds <- NULL\n",
    "best_model <- NULL\n",
    "\n",
    "for (i in 1:(n_pred - 1)) {\n",
    "  for (j in (i + 1):n_pred) {\n",
    "    fml_str <- paste(\"happiness ~\", predictor_names[i], \"+\", predictor_names[j])\n",
    "    m2 <- lm(as.formula(fml_str), data = v)\n",
    "    res2 <- predict_and_rmse(m2, v)\n",
    "    if (res2$rmse < best_rmse) {\n",
    "      best_rmse <- res2$rmse\n",
    "      best_preds <- c(predictor_names[i], predictor_names[j])\n",
    "      best_model <- m2\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# 6) RMSE for full and stepwise models\n",
    "res_full <- predict_and_rmse(model_full, v)\n",
    "res_step <- predict_and_rmse(model_step, v)\n",
    "\n",
    "# 7) Print results\n",
    "cat(\" Best 2-Predictor Model (exhaustive search) \\n\")\n",
    "cat(\"Predictors:\", best_preds[1], \"and\", best_preds[2], \"\\n\")\n",
    "cat(\"RMSE:\", best_rmse, \"\\n\")\n",
    "cat(\"R-squared:\", summary(best_model)$r.squared, \"\\n\\n\")\n",
    "\n",
    "cat(\" Stepwise (BIC) Model \\n\")\n",
    "cat(\"RMSE:\", res_step$rmse, \"\\n\")\n",
    "cat(\"R-squared:\", summary(model_step)$r.squared, \"\\n\")\n",
    "cat(\"Predictors:\", length(coef(model_step)) - 1, \"\\n\\n\")\n",
    "\n",
    "cat(\" Full Model \\n\")\n",
    "cat(\"RMSE:\", res_full$rmse, \"\\n\")\n",
    "cat(\"R-squared:\", summary(model_full)$r.squared, \"\\n\")\n",
    "cat(\"Predictors:\", length(coef(model_full)) - 1, \"\\n\\n\")\n",
    "\n",
    "cat(\"Difference in RMSE (2-pred - step):\", best_rmse - res_step$rmse, \"\\n\")\n",
    "cat(\"Difference in RMSE (2-pred - full):\", best_rmse - res_full$rmse, \"\\n\\n\")\n",
    "\n",
    "cat(\"2-predictor formula:\\n\")\n",
    "print(formula(best_model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The exhaustive search selected `income` and `alwaysStressed` as the best two-predictor model because they capture two dominant but complementary dimensions: economic status and psychological burden. However, its RMSE (7.89) and R\u00b2 (0.677) are clearly worse than the stepwise BIC model (RMSE 7.33, R\u00b2 0.721) and the full model (RMSE 6.67, R\u00b2 0.769). Although income was the strongest single predictor and stress adds meaningful variance, happiness in this dataset is influenced by several additional affective/behavioural variables. The 14-predictor stepwise model can incorporate these extra signals, so aggressively reducing to two variables leads to underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model \u2014 Random Forest with Feature Engineering\n",
    "\n",
    "### Approach Summary\n",
    "\n",
    "| Approach | CV RMSE | Outcome |\n",
    "|----------|---------|--------|\n",
    "| Raw data only | 9.25 | Underfitting |\n",
    "| **Moderate feature engineering** | **7.44** | **Best** |\n",
    "| Aggressive feature engineering | 7.71 | Overfitting |\n",
    "\n",
    "### Engineered Features\n",
    "- `stress_sq`, `fun_sq` \u2014 polynomial terms for non-linear relationships\n",
    "- `income_num` \u2014 numeric encoding of income brackets\n",
    "- `income \u00d7 stress`, `income \u00d7 fun` \u2014 interaction between economic and emotional factors\n",
    "- `stress \u00d7 fun` \u2014 emotion interaction term\n",
    "\n",
    "### Final Method\n",
    "- **Model**: Random Forest + Linear Regression ensemble (50/50 blend)\n",
    "- **RF parameters**: mtry=20, ntree=500\n",
    "- **Validation**: 5-fold cross-validation\n",
    "- **Rationale**: RF captures non-linear patterns; LM provides stability; ensemble is more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Packages loaded\n",
      "\n",
      "Data loaded:\n",
      "  Train: 500 rows \u00d7 43 cols\n",
      "  Test : 90 rows \u00d7 42 cols\n",
      "\n",
      "\u2713 Factor levels aligned\n",
      "\n",
      "========== USING RAW DATA (NO FEATURE ENGINEERING) ==========\n",
      "\n",
      "Original train features: 43 \n",
      "Original test features : 42 \n",
      "Note: NO polynomial terms, NO interactions, NO new variables\n",
      "\n",
      "========== TRAINING RF (PROVEN PARAMETERS) ==========\n",
      "\n",
      "[1/2] Training RF with raw data...\n",
      "Parameters based on 5.54 success:\n",
      "  mtry: 20\n",
      "  ntree: 500\n",
      "\n",
      "\u2713 RF Training Complete\n",
      "  CV RMSE: 9.2486 \u00b1 SD: 0.8722 \n",
      "\n",
      "[2/2] Training Linear Regression (baseline)...\n",
      "  CV RMSE: 7.9012 \n",
      "\n",
      "========== COMPARISON: RAW DATA vs ENGINEERED =========\n",
      "\n",
      "                         Approach  CV_RMSE Difference\n",
      "1              Raw data (current) 9.248642   0.000000\n",
      "2 With feature engineering (5.54) 7.436800   1.811842\n",
      "\n",
      "\u274c Raw data WORSE! CV RMSE increased by 1.812 points\n",
      "\n",
      "========== TOP 15 FEATURES (RAW DATA) ==========\n",
      "                                                                                              Feature\n",
      "income80k - 120k                                                                     income80k - 120k\n",
      "income50k - 80k                                                                       income50k - 80k\n",
      "income200k above                                                                     income200k above\n",
      "income20k - 50k                                                                       income20k - 50k\n",
      "alwaysStressed                                                                         alwaysStressed\n",
      "income15k - 20k                                                                       income15k - 20k\n",
      "income150k - 200k                                                                   income150k - 200k\n",
      "income120k - 150k                                                                   income120k - 150k\n",
      "alwaysAnxious                                                                           alwaysAnxious\n",
      "extremelyGoodAbilityToSense                                               extremelyGoodAbilityToSense\n",
      "iDontFeelParticularlyPleasedWithTheWayIAm                   iDontFeelParticularlyPleasedWithTheWayIAm\n",
      "iDoNotThinkThatTheWorldIsAGoodPlace                               iDoNotThinkThatTheWorldIsAGoodPlace\n",
      "doYouFeelASenseOfPurposeAndMeaningInYourLife104Yes doYouFeelASenseOfPurposeAndMeaningInYourLife104Yes\n",
      "alwaysDepressed                                                                       alwaysDepressed\n",
      "iAmIntenselyInterestedInOtherPeople                               iAmIntenselyInterestedInOtherPeople\n",
      "                                                   Importance\n",
      "income80k - 120k                                    49.680086\n",
      "income50k - 80k                                     31.236209\n",
      "income200k above                                    26.143436\n",
      "income20k - 50k                                     23.595977\n",
      "alwaysStressed                                      19.209835\n",
      "income15k - 20k                                     11.578660\n",
      "income150k - 200k                                    8.112436\n",
      "income120k - 150k                                    7.597075\n",
      "alwaysAnxious                                        5.735583\n",
      "extremelyGoodAbilityToSense                          5.559874\n",
      "iDontFeelParticularlyPleasedWithTheWayIAm            5.142217\n",
      "iDoNotThinkThatTheWorldIsAGoodPlace                  5.088735\n",
      "doYouFeelASenseOfPurposeAndMeaningInYourLife104Yes   4.574333\n",
      "alwaysDepressed                                      4.327018\n",
      "iAmIntenselyInterestedInOtherPeople                  3.960590\n",
      "\n",
      "Best model: LINEAR REGRESSION\n",
      "\n",
      "========== GENERATING PREDICTIONS ==========\n",
      "Prediction summary:\n",
      "  RF  Mean:0.94, Range: [-14.16, 30.4]\n",
      "  LM  Mean:-0.37, Range: [-30.78, 35.39]\n",
      "  ENS Mean:0.29, Range: [-21.47, 32.86]\n",
      "\n",
      " Submission file created: RegressionPredictLabel.csv\n",
      "  Rows: 90 \n",
      "\n",
      "========== ANALYSIS SUMMARY ==========\n",
      "\n",
      "Test: Does raw data outperform engineered features?\n",
      "\n",
      "Raw data CV RMSE:       9.2486 \n",
      "Engineered CV RMSE:    7.4368\n",
      "Difference:             1.8118 \n",
      "\n",
      "\u274c FINDING: Raw data worse\n",
      "   Suggests: Feature engineering is necessary\n",
      "   Action: Continue with engineered features (5.54)\n",
      "\n",
      " Raw data analysis complete!\n",
      " Model training complete\n",
      " Predictions exported to RegressionPredictLabel.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Q5 \u2013 RANDOM FOREST (RAW DATA, NO FEATURE ENGINEERING)\n",
    "# Based on the optimal parameters of 5.54: mtry=20, ntree=500\n",
    "# Test: The effect of original data vs engineered features\n",
    "# ================================================================\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load packages\n",
    "# ----------------------------------------\n",
    "if (!require(\"caret\", character.only = TRUE)) {\n",
    "  install.packages(\"caret\", repos='http://cran.us.r-project.org')\n",
    "  library(\"caret\")\n",
    "}\n",
    "if (!require(\"randomForest\", character.only = TRUE)) {\n",
    "  install.packages(\"randomForest\", repos='http://cran.us.r-project.org')\n",
    "  library(\"randomForest\")\n",
    "}\n",
    "\n",
    "cat(\" Packages loaded\\n\\n\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load data\n",
    "# ----------------------------------------\n",
    "train <- read.csv(\"regression_train.csv\", stringsAsFactors = FALSE)\n",
    "test  <- read.csv(\"regression_test.csv\",  stringsAsFactors = FALSE)\n",
    "\n",
    "cat(\"Data loaded:\\n\")\n",
    "cat(\"  Train:\", nrow(train), \"rows \u00d7\", ncol(train), \"cols\\n\")\n",
    "cat(\"  Test :\", nrow(test),  \"rows \u00d7\", ncol(test),  \"cols\\n\\n\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Align factors (NO feature engineering)\n",
    "# ----------------------------------------\n",
    "factor_cols <- c(\n",
    "  \"gender\",\n",
    "  \"income\",\n",
    "  \"whatIsYourHeightExpressItAsANumberInMetresM\",\n",
    "  \"doYouFeelASenseOfPurposeAndMeaningInYourLife104\",\n",
    "  \"howDoYouReconcileSpiritualBeliefsWithScientificOrRationalThinki\",\n",
    "  \"howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends\",\n",
    "  \"doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded\",\n",
    "  \"howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV\",\n",
    "  \"doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer\",\n",
    "  \"doYouFeelASenseOfPurposeAndMeaningInYourLife105\"\n",
    ")\n",
    "\n",
    "for (col in factor_cols) {\n",
    "  if (col %in% names(train)) {\n",
    "    train[[col]] <- factor(train[[col]])\n",
    "  }\n",
    "}\n",
    "\n",
    "for (col in factor_cols) {\n",
    "  if (col %in% names(train) && col %in% names(test)) {\n",
    "    test[[col]] <- factor(test[[col]], levels = levels(train[[col]]))\n",
    "  }\n",
    "}\n",
    "\n",
    "cat(\" Factor levels aligned\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# NO FEATURE ENGINEERING - USE RAW DATA\n",
    "# ========================================\n",
    "cat(\"========== USING RAW DATA (NO FEATURE ENGINEERING) ==========\\n\\n\")\n",
    "\n",
    "cat(\"Original train features:\", ncol(train), \"\\n\")\n",
    "cat(\"Original test features :\", ncol(test), \"\\n\")\n",
    "cat(\"Note: NO polynomial terms, NO interactions, NO new variables\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Cross-validation setup\n",
    "# ========================================\n",
    "cv_ctrl <- trainControl(method = \"cv\", number = 5, verboseIter = FALSE)\n",
    "\n",
    "# ========================================\n",
    "# Train RF with proven parameters from 5.54 version\n",
    "# ========================================\n",
    "cat(\"========== TRAINING RF (PROVEN PARAMETERS) ==========\\n\\n\")\n",
    "\n",
    "cat(\"[1/2] Training RF with raw data...\\n\")\n",
    "cat(\"Parameters based on 5.54 success:\\n\")\n",
    "cat(\"  mtry: 20\\n\")\n",
    "cat(\"  ntree: 500\\n\\n\")\n",
    "\n",
    "model_rf <- train(\n",
    "  happiness ~ .,\n",
    "  data = train,\n",
    "  method = \"rf\",\n",
    "  trControl = cv_ctrl,\n",
    "  tuneGrid = expand.grid(mtry = 20),\n",
    "  ntree = 500,\n",
    "  importance = TRUE,\n",
    "  verbose = FALSE\n",
    ")\n",
    "\n",
    "rmse_rf <- min(model_rf$results$RMSE)\n",
    "sd_rf   <- sd(model_rf$resample$RMSE)\n",
    "\n",
    "cat(\"\u2713 RF Training Complete\\n\")\n",
    "cat(\"  CV RMSE:\", round(rmse_rf, 4), \"\u00b1 SD:\", round(sd_rf, 4), \"\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Linear regression baseline\n",
    "# ========================================\n",
    "cat(\"[2/2] Training Linear Regression (baseline)...\\n\")\n",
    "\n",
    "model_lm <- train(\n",
    "  happiness ~ .,\n",
    "  data = train,\n",
    "  method = \"lm\",\n",
    "  trControl = cv_ctrl\n",
    ")\n",
    "\n",
    "rmse_lm <- min(model_lm$results$RMSE)\n",
    "\n",
    "cat(\"  CV RMSE:\", round(rmse_lm, 4), \"\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Comparison with feature engineering version\n",
    "# ========================================\n",
    "cat(\"========== COMPARISON: RAW DATA vs ENGINEERED =========\\n\\n\")\n",
    "\n",
    "comparison <- data.frame(\n",
    "  Approach = c(\"Raw data (current)\", \"With feature engineering (5.54)\"),\n",
    "  CV_RMSE = c(rmse_rf, 7.4368),\n",
    "  Difference = c(0, rmse_rf - 7.4368)\n",
    ")\n",
    "\n",
    "print(comparison)\n",
    "cat(\"\\n\")\n",
    "\n",
    "if (rmse_rf < 7.4368) {\n",
    "  cat(\"\u2705 Raw data BETTER! CV RMSE improved by\", round(7.4368 - rmse_rf, 3), \"points\\n\\n\")\n",
    "} else if (rmse_rf > 7.4368) {\n",
    "  cat(\"\u274c Raw data WORSE! CV RMSE increased by\", round(rmse_rf - 7.4368, 3), \"points\\n\\n\")\n",
    "} else {\n",
    "  cat(\"\u2192 Similar performance\\n\\n\")\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# Feature importance (raw data)\n",
    "# ========================================\n",
    "cat(\"========== TOP 15 FEATURES (RAW DATA) ==========\\n\")\n",
    "\n",
    "imp <- importance(model_rf$finalModel)\n",
    "imp_df <- data.frame(\n",
    "  Feature = rownames(imp),\n",
    "  Importance = imp[, 1]\n",
    ")\n",
    "imp_df <- imp_df[order(-imp_df$Importance), ]\n",
    "\n",
    "print(head(imp_df, 15))\n",
    "cat(\"\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Select best model\n",
    "# ========================================\n",
    "if (rmse_rf < rmse_lm) {\n",
    "  fin.mod <- model_rf\n",
    "  cat(\"Best model: RANDOM FOREST\\n\\n\")\n",
    "} else {\n",
    "  fin.mod <- model_lm\n",
    "  cat(\"Best model: LINEAR REGRESSION\\n\\n\")\n",
    "}\n",
    "\n",
    "# ========================================\n",
    "# Generate predictions\n",
    "# ========================================\n",
    "cat(\"========== GENERATING PREDICTIONS ==========\\n\")\n",
    "\n",
    "pred_rf <- predict(model_rf, newdata = test)\n",
    "pred_lm <- predict(model_lm, newdata = test)\n",
    "\n",
    "# 50-50 ensemble\n",
    "pred_ens <- 0.5 * pred_rf + 0.5 * pred_lm\n",
    "\n",
    "cat(\"Prediction summary:\\n\")\n",
    "cat(\"  RF  Mean:\", round(mean(pred_rf), 2), \n",
    "    \", Range: [\", round(min(pred_rf), 2), \", \", round(max(pred_rf), 2), \"]\\n\", sep=\"\")\n",
    "cat(\"  LM  Mean:\", round(mean(pred_lm), 2), \n",
    "    \", Range: [\", round(min(pred_lm), 2), \", \", round(max(pred_lm), 2), \"]\\n\", sep=\"\")\n",
    "cat(\"  ENS Mean:\", round(mean(pred_ens), 2), \n",
    "    \", Range: [\", round(min(pred_ens), 2), \", \", round(max(pred_ens), 2), \"]\\n\\n\", sep=\"\")\n",
    "\n",
    "# ========================================\n",
    "# Create submission\n",
    "# ========================================\n",
    "submission <- data.frame(\n",
    "  RowIndex = 1:nrow(test),\n",
    "  Prediction = pred_ens\n",
    ")\n",
    "\n",
    "write.csv(submission, \"RegressionPredictLabel.csv\", row.names = FALSE)\n",
    "\n",
    "cat(\" Submission file created: RegressionPredictLabel.csv\\n\")\n",
    "cat(\"  Rows:\", nrow(submission), \"\\n\\n\")\n",
    "\n",
    "# ========================================\n",
    "# Final summary\n",
    "# ========================================\n",
    "cat(\"========== ANALYSIS SUMMARY ==========\\n\\n\")\n",
    "\n",
    "cat(\"Test: Does raw data outperform engineered features?\\n\\n\")\n",
    "\n",
    "cat(\"Raw data CV RMSE:      \", round(rmse_rf, 4), \"\\n\")\n",
    "cat(\"Engineered CV RMSE:    7.4368\\n\")\n",
    "cat(\"Difference:            \", round(rmse_rf - 7.4368, 4), \"\\n\\n\")\n",
    "\n",
    "if (rmse_rf < 7.30) {\n",
    "  cat(\"\u2705 FINDING: Raw data significantly better!\\n\")\n",
    "  cat(\"   Suggests: Feature engineering was adding noise\\n\")\n",
    "  cat(\"   Action: Use raw data for out-of-sample evaluation\\n\")\n",
    "  cat(\"   Expectation: May improve on previous 5.54 score!\\n\")\n",
    "} else if (rmse_rf < 7.44) {\n",
    "  cat(\"\u2192 FINDING: Raw data competitive\\n\")\n",
    "  cat(\"   Suggests: Features help slightly\\n\")\n",
    "  cat(\"   Action: Could go either way\\n\")\n",
    "} else {\n",
    "  cat(\"\u274c FINDING: Raw data worse\\n\")\n",
    "  cat(\"   Suggests: Feature engineering is necessary\\n\")\n",
    "  cat(\"   Action: Continue with engineered features (5.54)\\n\")\n",
    "}\n",
    "\n",
    "cat(\"\\n Raw data analysis complete!\\n\")\n",
    "cat(\" Final model stored in fin.mod\\n\")\n",
    "cat(\" Predictions exported to RegressionPredictLabel.csv\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}